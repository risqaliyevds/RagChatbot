# Ports
EMBEDDING_PORT=11445
CHAT_MODEL_PORT=11444

# vLLM API Configuration
# vLLM Configuration
VLLM_API_KEY=EMPTY
CHAT_MODEL=google/gemma-3-12b-it
# VLLM_EMBEDDING_ENDPOINT=http://localhost:11445/v1  # Commented out to use local embeddings
VLLM_CHAT_ENDPOINT=http://localhost:11444/v1

# Qdrant Configuration
# For local file storage (no Docker required):
# QDRANT_PATH=./qdrant_storage
QDRANT_COLLECTION_NAME=rag_documents

# For Docker/remote Qdrant (uncomment to use):
QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY=your_api_key_if_needed

# Qdrant Vector Configuration
# Vector dimension - will be auto-detected from embedding model if not set
# Common dimensions: 384 (sentence-transformers/all-MiniLM-L6-v2), 768 (BERT-base), 1024 (multilingual-e5-large-instruct)
QDRANT_VECTOR_SIZE=1024  # Auto-detect from embedding model
# Distance metric: COSINE, EUCLID, DOT
QDRANT_DISTANCE=COSINE
# Force recreate collection if dimension mismatch (true/false)
QDRANT_FORCE_RECREATE=true
# Vector storage on disk (true/false) - saves RAM but slower access
QDRANT_ON_DISK=false

# Document Configuration
DOCUMENT_URL=https://docs.vllm.ai/en/latest/getting_started/quickstart.html
DOCUMENTS_PATH=/mnt/mata/chatbot/documents

# Model Configuration
EMBEDDING_MODEL=intfloat/multilingual-e5-large-instruct
CHAT_MODEL=google/gemma-3-12b-it

# Retrieval Configuration
TOP_K=3
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# FastAPI Configuration
HOST=0.0.0.0
PORT=8080 